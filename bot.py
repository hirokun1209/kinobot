import os
import io
import re
import base64
import asyncio
from typing import List, Tuple, Dict

import discord
from discord.ext import commands
from PIL import Image, ImageDraw, ImageOps

# Google Vision
from google.cloud import vision

# OpenAI (official SDK v1)
from openai import OpenAI

# ---------------------------
# ENV/bootstrap
# ---------------------------
DISCORD_TOKEN = os.environ.get("DISCORD_TOKEN", "")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
GOOGLE_CLOUD_VISION_JSON = os.environ.get("GOOGLE_CLOUD_VISION_JSON", "")

if not DISCORD_TOKEN or not OPENAI_API_KEY:
    raise RuntimeError("DISCORD_TOKEN / OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")

# Google Vision èªè¨¼ï¼ˆJSONæ–‡å­—åˆ—â†’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
if GOOGLE_CLOUD_VISION_JSON:
    cred_path = "/tmp/gcv_credentials.json"
    with open(cred_path, "w", encoding="utf-8") as f:
        f.write(GOOGLE_CLOUD_VISION_JSON)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = cred_path

gcv_client = vision.ImageAnnotatorClient()
oai_client = OpenAI(api_key=OPENAI_API_KEY)

# ---------------------------
# Discord Bot
# ---------------------------
intents = discord.Intents.default()
intents.message_content = True
bot = commands.Bot(command_prefix="!", intents=intents)

# ---------------------------
# Parameters / Config
# ---------------------------

# ã‚¹ãƒ©ã‚¤ã‚¹å¢ƒç•Œï¼ˆ%ï¼‰
CUTS = [7.51, 11.63, 25.21, 29.85, 33.62, 38.41, 61.90]  # ä¸Šã‹ã‚‰ã®å¢ƒç•Œï¼…
# æ®‹ã™ãƒ–ãƒ­ãƒƒã‚¯ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
KEEP = [2, 4, 6, 7]

# æ¨ªæ–¹å‘ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå·¦/å³ã‚’ï¼…ã§ã‚«ãƒƒãƒˆï¼‰
TRIM_RULES = {
    7: (20.0, 50.0),   # é§é¨ãƒŠãƒ³ãƒãƒ¼ï¼‹å…æˆ¦æ™‚é–“
    6: (32.48, 51.50), # ã‚µãƒ¼ãƒãƒ¼ç•ªå·
    4: (44.0, 20.19),  # åœæˆ¦çµ‚äº†
    2: (75.98, 10.73), # æ™‚è¨ˆ
}

# æ­£è¦è¡¨ç¾
RE_IMMUNE = re.compile(r"å…\s*æˆ¦\s*ä¸­")                         # ã€Œå…æˆ¦ä¸­ã€
RE_TITLE  = re.compile(r"è¶Š\s*åŸŸ\s*é§\s*[é¨æ©Ÿ]\s*å ´")             # ã€Œè¶ŠåŸŸé§é¨å ´/è¶ŠåŸŸé§æ©Ÿå ´ã€
RE_TIME   = re.compile(r"\d{1,2}[:ï¼š]\d{2}(?:[:ï¼š]\d{2})?")       # 05:53 / 01:02:13 ãªã©

# ä½™ç™½ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
MARGIN_AFTER_INFO = 0               # å…æˆ¦ä¸­ã‚„æ™‚é–“ã®â€œä¸‹ç«¯ã‹ã‚‰â€è©°ã‚é–‹å§‹
MARGIN_BEFORE_NEXT_TITLE = 0        # æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ã®â€œä¸Šç«¯ã¾ã§â€è©°ã‚ã‚‹

# ---------------------------
# Helpers
# ---------------------------

def load_image_from_bytes(data: bytes) -> Image.Image:
    im = Image.open(io.BytesIO(data))
    im = ImageOps.exif_transpose(im)  # EXIFã®å‘ãã‚’è£œæ­£
    return im.convert("RGBA")

def slice_exact_7(im: Image.Image, cuts_pct: List[float]) -> List[Image.Image]:
    """å¢ƒç•Œï¼…ã‹ã‚‰7ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ï¼ˆ1..7ï¼‰"""
    w, h = im.size
    boundaries = [int(round(h * p / 100.0)) for p in cuts_pct]
    y = [0] + boundaries + [h]
    parts = []
    for i in range(7):
        parts.append(im.crop((0, y[i], w, y[i+1])))
    return parts

def trim_lr_percent(im: Image.Image, left_pct: float, right_pct: float) -> Image.Image:
    w, h = im.size
    left = int(round(w * left_pct / 100.0))
    right = w - int(round(w * right_pct / 100.0))
    left = max(0, min(left, w - 1))
    right = max(left + 1, min(right, w))
    return im.crop((left, 0, right, h))

def google_ocr_word_boxes(pil_im: Image.Image) -> List[Tuple[str, Tuple[int,int,int,int]]]:
    """Google Vision ã§ word å˜ä½ã®æ–‡å­—ã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’è¿”ã™ (text, (x1,y1,x2,y2))"""
    buf = io.BytesIO()
    pil_im.convert("RGB").save(buf, format="JPEG", quality=95)
    image = vision.Image(content=buf.getvalue())

    response = gcv_client.document_text_detection(image=image)
    if response.error.message:
        raise RuntimeError(response.error.message)

    words: List[Tuple[str, Tuple[int,int,int,int]]] = []
    if not response.full_text_annotation.pages:
        return words

    for page in response.full_text_annotation.pages:
        for block in page.blocks:
            for para in block.paragraphs:
                for word in para.words:
                    txt = "".join([s.text for s in word.symbols])
                    xs = [v.x for v in word.bounding_box.vertices]
                    ys = [v.y for v in word.bounding_box.vertices]
                    x1, x2 = min(xs), max(xs)
                    y1, y2 = min(ys), max(ys)
                    words.append((txt, (x1, y1, x2, y2)))
    return words

def _words_to_lines(words: List[Tuple[str, Tuple[int,int,int,int]]],
                    y_tol: int) -> List[Tuple[str, Tuple[int,int,int,int]]]:
    """words -> è¡Œã‚¯ãƒ©ã‚¹ã‚¿ (joined_text, bbox)"""
    if not words:
        return []
    words = sorted(words, key=lambda w: (((w[1][1] + w[1][3]) // 2), w[1][0]))
    lines = []
    cur = []

    def flush():
        if not cur:
            return
        xs = [b[0] for _, b in cur] + [b[2] for _, b in cur]
        ys = [b[1] for _, b in cur] + [b[3] for _, b in cur]
        x1, x2 = min(xs), max(xs)
        y1, y2 = min(ys), max(ys)
        text = "".join(t for t, _ in sorted(cur, key=lambda w: w[1][0]))
        lines.append((text, (x1, y1, x2, y2)))

    base_y = None
    for t, (x1, y1, x2, y2) in words:
        yc = (y1 + y2) // 2
        if base_y is None or abs(yc - base_y) > y_tol:
            flush()
            base_y = yc
            cur = [(t, (x1, y1, x2, y2))]
        else:
            cur.append((t, (x1, y1, x2, y2)))
    flush()
    return sorted(lines, key=lambda it: it[1][1])

def google_ocr_line_boxes(pil_im: Image.Image, y_tol: int = None) -> List[Tuple[str, Tuple[int,int,int,int]]]:
    """è¡Œãƒœãƒƒã‚¯ã‚¹ã¸å¤‰æ›"""
    words = google_ocr_word_boxes(pil_im)
    if not words:
        return []
    if y_tol is None:
        y_tol = max(8, int(round(pil_im.height * 0.01)))
    return _words_to_lines(words, y_tol)

def remove_vertical_segments(im: Image.Image, segments: List[Tuple[int, int]]) -> Image.Image:
    """
    ç”»åƒã‹ã‚‰ [top, bottom) ã®ç¸¦åŒºé–“ã‚’å‰Šé™¤ã—ã¦è©°ã‚ã‚‹ã€‚
    segments ã¯ç”»åƒå†…yåº§æ¨™ã€‚é‡è¤‡/éš£æ¥ã¯è‡ªå‹•ãƒãƒ¼ã‚¸ã€‚
    """
    if not segments:
        return im.copy()

    w, h = im.size
    # æ­£è¦åŒ–ï¼†ãƒãƒ¼ã‚¸
    segs = []
    for t, b in segments:
        t = max(0, min(h, t))
        b = max(0, min(h, b))
        if b <= t:
            continue
        segs.append((t, b))
    if not segs:
        return im.copy()

    segs.sort()
    merged = []
    cur_t, cur_b = segs[0]
    for t, b in segs[1:]:
        if t <= cur_b:  # é‡è¤‡/éš£æ¥
            cur_b = max(cur_b, b)
        else:
            merged.append((cur_t, cur_b))
            cur_t, cur_b = t, b
    merged.append((cur_t, cur_b))

    # æ®‹ã™åŒºé–“ã‚’ä¸Šã‹ã‚‰è²¼ã‚Šåˆã‚ã›
    keep_ranges = []
    cursor = 0
    for t, b in merged:
        if t > cursor:
            keep_ranges.append((cursor, t))
        cursor = b
    if cursor < h:
        keep_ranges.append((cursor, h))

    # æ–°ã—ã„é«˜ã•
    new_h = sum(b - a for a, b in keep_ranges)
    canvas = Image.new("RGBA", (w, new_h), (0, 0, 0, 0))
    y = 0
    for a, b in keep_ranges:
        piece = im.crop((0, a, w, b))
        canvas.paste(piece, (0, y))
        y += (b - a)
    return canvas

def compress_block7_by_rules(pil_im: Image.Image) -> Image.Image:
    """
    7ç•ªãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã€Œè©°ã‚ã€å‡¦ç†ï¼š
      ãƒ»ã‚¿ã‚¤ãƒˆãƒ«è¡Œï¼ˆè¶ŠåŸŸé§é¨/é§æ©Ÿå ´ï¼‰ã‚’è¦‹å‡ºã—ã¨ã—ã¦æŠ½å‡º
      ãƒ»å„ã‚¿ã‚¤ãƒˆãƒ«iã®ä¸‹ç«¯ã€œã‚¿ã‚¤ãƒˆãƒ«i+1ã®ä¸Šç«¯ã‚’â€œ1ãƒ–ãƒ­ãƒƒã‚¯â€
      ãƒ»ãƒ–ãƒ­ãƒƒã‚¯å†…ã«ã€Œå…æˆ¦ä¸­ã€ or æ™‚é–“(05:53/01:02:13ç­‰)ãŒã‚ã‚Œã°
         ãã®â€œæœ€å¾Œã«å‡ºã‚‹è¡Œâ€ã®ä¸‹ç«¯+MARGIN_AFTER_INFO ã€œ æ¬¡ã‚¿ã‚¤ãƒˆãƒ«ä¸Šç«¯-MARGIN_BEFORE_NEXT_TITLE ã‚’å‰Šé™¤
      ãƒ»ã©ã¡ã‚‰ã‚‚ç„¡ã‘ã‚Œã° ã‚¿ã‚¤ãƒˆãƒ«ä¸‹ç«¯ã€œæ¬¡ã‚¿ã‚¤ãƒˆãƒ«ä¸Šç«¯ã‚’å‰Šé™¤
      ãƒ»æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã®çµ‚ç«¯ã¯ç”»åƒæœ€ä¸‹éƒ¨
    """
    im = pil_im.copy()
    w, h = im.size

    lines = google_ocr_line_boxes(im)

    # è¡Œã®æŠ½å‡º
    titles: List[Tuple[int, int]] = []
    info_rows: List[Tuple[int, int]] = []
    for text, (x1, y1, x2, y2) in lines:
        t_no_space = text.replace(" ", "")
        if RE_TITLE.search(t_no_space):
            titles.append((y1, y2))
        if RE_IMMUNE.search(t_no_space) or RE_TIME.search(text):
            info_rows.append((y1, y2))
    titles.sort(key=lambda p: p[0])
    info_rows.sort(key=lambda p: p[0])

    if not titles:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒæ¤œå‡ºã§ããªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™ï¼ˆå®‰å…¨å´ï¼‰
        return im

    # å‰Šé™¤ã™ã¹ãç¸¦åŒºé–“ã‚’é›†ã‚ã‚‹
    remove_segments: List[Tuple[int, int]] = []
    for i, (ty1, ty2) in enumerate(titles):
        start = ty2
        end = titles[i + 1][0] if i + 1 < len(titles) else h
        if end <= start:
            continue

        # ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ä¸€ç•ªä¸‹ã«ç¾ã‚Œã‚‹ info è¡Œã®ä¸‹ç«¯ã‚’ä½¿ã†
        last_info_bottom = None
        for iy1, iy2 in info_rows:
            if start <= iy1 < end:
                last_info_bottom = iy2

        top = (last_info_bottom + MARGIN_AFTER_INFO) if last_info_bottom is not None else start
        bottom = max(top, end - MARGIN_BEFORE_NEXT_TITLE)
        if bottom > top:
            remove_segments.append((top, bottom))

    # ãã®åŒºé–“ã‚’å‰Šé™¤ã—ã€ä¸Šã«è©°ã‚ãŸç”»åƒã‚’è¿”ã™
    return remove_vertical_segments(im, remove_segments)

def hstack(im_left: Image.Image, im_right: Image.Image, gap: int = 8, bg=(0,0,0,0)) -> Image.Image:
    """å·¦å³ã«çµåˆï¼ˆé«˜ã•ã¯å¤§ãã„æ–¹ã«åˆã‚ã›ä¸­å¤®å¯„ã›ï¼‰"""
    h = max(im_left.height, im_right.height)
    w = im_left.width + gap + im_right.width
    canvas = Image.new("RGBA", (w, h), bg)
    y1 = (h - im_left.height) // 2
    y2 = (h - im_right.height) // 2
    canvas.paste(im_left, (0, y1))
    canvas.paste(im_right, (im_left.width + gap, y2))
    return canvas

def vstack(images: List[Image.Image], gap: int = 8, bg=(0,0,0,0)) -> Image.Image:
    """ç¸¦çµåˆï¼ˆå¹…ã¯æœ€å¤§ã«åˆã‚ã›ä¸­å¤®å¯„ã›ï¼‰"""
    if not images:
        raise ValueError("no images to stack")
    max_w = max(img.width for img in images)
    total_h = sum(img.height for img in images) + gap * (len(images) - 1)
    canvas = Image.new("RGBA", (max_w, total_h), bg)
    y = 0
    for img in images:
        x = (max_w - img.width) // 2
        canvas.paste(img, (x, y))
        y += img.height + gap
    return canvas

def openai_ocr_png(pil_im: Image.Image) -> Tuple[str, bytes]:
    """OpenAI ã¸ç”»åƒOCRä¾é ¼ã€‚è¿”ã‚Šå€¤: (ãƒ†ã‚­ã‚¹ãƒˆ, é€ã£ãŸPNGãƒã‚¤ãƒˆåˆ—)"""
    buf = io.BytesIO()
    pil_im.convert("RGB").save(buf, format="PNG")
    png_bytes = buf.getvalue()

    b64 = base64.b64encode(png_bytes).decode("utf-8")
    resp = oai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": "ä»¥ä¸‹ã®ç”»åƒã«å†™ã£ã¦ã„ã‚‹æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’ã€ãã®ã¾ã¾èª­ã¿å–ã£ã¦ãã ã•ã„ï¼ˆæ”¹è¡Œã¨æ•°å­—ã‚‚ä¿æŒï¼‰ã€‚"},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}}
            ]
        }],
        temperature=0.0,
    )
    text = resp.choices[0].message.content.strip()
    return text, png_bytes

def process_image_pipeline(pil_im: Image.Image) -> Tuple[Image.Image, str, bytes]:
    """
    æŒ‡å®šã®ã‚¹ãƒ©ã‚¤ã‚¹â†’ãƒˆãƒªãƒ â†’ï¼ˆ7ã‚’è©°ã‚å‡¦ç†ï¼‰â†’åˆæˆâ†’OpenAI OCR
    æˆ»ã‚Šå€¤: (æœ€çµ‚åˆæˆç”»åƒ, OpenAI OCRãƒ†ã‚­ã‚¹ãƒˆ, OpenAIã¸é€ã£ãŸç”»åƒbytes)
    """
    parts = slice_exact_7(pil_im, CUTS)

    # 1..7 ã®ã†ã¡ 2/4/6/7 ã‚’æ®‹ã™
    kept: Dict[int, Image.Image] = {}
    for idx in KEEP:
        block = parts[idx - 1]
        l_pct, r_pct = TRIM_RULES[idx]
        kept[idx] = trim_lr_percent(block, l_pct, r_pct)

    # 7ç•ªã‚’â€œé»’å¡—ã‚Šç›¸å½“ã®åŒºé–“ã‚’å‰Šé™¤ã—ã¦è©°ã‚ã‚‹â€
    kept[7] = compress_block7_by_rules(kept[7])

    # 6ã®å³éš£ã« 2 ã‚’æ¨ªä¸¦ã³ï¼ˆæ™‚è¨ˆã¯ã‚µãƒ¼ãƒãƒ¼ç•ªå·ã®éš£ï¼‰
    top_row = hstack(kept[6], kept[2], gap=8)

    # ç¸¦ã« 4ã€7 ã‚’ä¸‹ã¸
    final_img = vstack([top_row, kept[4], kept[7]], gap=10)

    # OpenAI OCR
    oai_text, sent_png = openai_ocr_png(final_img)
    return final_img, oai_text, sent_png

# ---------------------------
# Discord command
# ---------------------------

@bot.command(name="oaiocr", help="ç”»åƒã‚’æ·»ä»˜ã—ã¦å®Ÿè¡Œã€‚å‡¦ç†â†’è©°ã‚â†’OpenAI OCR ã¾ã§è¡Œã„ã¾ã™ã€‚")
async def oaiocr(ctx: commands.Context):
    try:
        if not ctx.message.attachments:
            await ctx.reply("ç”»åƒã‚’æ·»ä»˜ã—ã¦ `!oaiocr` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return

        att: discord.Attachment = None
        for a in ctx.message.attachments:
            if a.content_type and a.content_type.startswith("image/"):
                att = a
                break
        if att is None:
            await ctx.reply("ç”»åƒã®æ·»ä»˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # å—ä¿¡ç›´å¾Œã«å³ãƒ¬ã‚¹
        await ctx.reply("è§£æä¸­â€¦")

        data = await att.read()
        pil = load_image_from_bytes(data)

        loop = asyncio.get_event_loop()
        final_img, oai_text, sent_png = await loop.run_in_executor(None, process_image_pipeline, pil)

        out_buf = io.BytesIO()
        final_img.convert("RGB").save(out_buf, format="PNG")
        out_buf.seek(0)

        sent_buf = io.BytesIO(sent_png); sent_buf.seek(0)

        files = [
            discord.File(out_buf, filename="result.png"),
            discord.File(sent_buf, filename="sent_to_openai.png"),
        ]
        await ctx.reply(content=f"OpenAI OCR çµæœ:\n```\n{oai_text}\n```", files=files)

    except Exception as e:
        await ctx.reply(f"ã‚¨ãƒ©ãƒ¼: {e}")

@bot.command(name="ping")
async def ping(ctx: commands.Context):
    await ctx.reply("pong ğŸ“")

if __name__ == "__main__":
    bot.run(DISCORD_TOKEN)